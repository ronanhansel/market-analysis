{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "543a2ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d3002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('results/stooq_merged.csv', parse_dates=['Date'])\n",
    "\n",
    "# 1. Rename the long News columns to something shorter\n",
    "df = df.rename(columns={\n",
    "    'MERGED_GDELT_STOOQ_ALIGNED_News_Sentiment': 'Sentiment_Tone',\n",
    "    'MERGED_GDELT_STOOQ_ALIGNED_News_Disagreement': 'Sentiment_Dispersion',\n",
    "    'MERGED_GDELT_STOOQ_ALIGNED_News_Volume': 'News_Volume'\n",
    "})\n",
    "\n",
    "# 2. Select only the core columns you need for the first model (S&P 500 Focus)\n",
    "cols_to_keep = [\n",
    "    'Date', \n",
    "    'SPX_Close', 'SPX_Volume',  # The Target Index\n",
    "    'Sentiment_Tone', 'Sentiment_Dispersion', 'News_Volume' # The Signals\n",
    "]\n",
    "\n",
    "df_clean = df[cols_to_keep].copy()\n",
    "df_clean = df_clean.set_index('Date').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ef25d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for Modeling. Columns available:\n",
      "['SPX_Close', 'SPX_Volume', 'Sentiment_Tone', 'Sentiment_Dispersion', 'News_Volume', 'Return_Daily', 'Target_NextDay_Return', 'Target_Direction']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Calculate Daily Log Returns (The \"Truth\")\n",
    "df_clean['Return_Daily'] = np.log(df_clean['SPX_Close'] / df_clean['SPX_Close'].shift(1))\n",
    "\n",
    "# 2. Create the TARGET (Next Day's Return)\n",
    "# We shift UP by 1. Row 't' now contains the return for 't+1'\n",
    "df_clean['Target_NextDay_Return'] = df_clean['Return_Daily'].shift(-1)\n",
    "\n",
    "# 3. Create Binary Target (Direction) - Optional but good for Classification\n",
    "# 1 if Up, 0 if Down\n",
    "df_clean['Target_Direction'] = (df_clean['Target_NextDay_Return'] > 0).astype(int)\n",
    "\n",
    "# 4. Handle Missing Values (The shifting creates NaNs at the end)\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "print(\"Ready for Modeling. Columns available:\")\n",
    "print(df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8dd5bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Sentiment_Tone  News_Volume  Target_NextDay_Return\n",
      "Sentiment_Tone               1.000000     0.312139              -0.000960\n",
      "News_Volume                  0.312139     1.000000              -0.006388\n",
      "Target_NextDay_Return       -0.000960    -0.006388               1.000000\n"
     ]
    }
   ],
   "source": [
    "correlation = df_clean[['Sentiment_Tone', 'News_Volume', 'Target_NextDay_Return']].corr()\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26224df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Features Created:\n",
      "            Sent_MA_7  Sent_Momentum  Weighted_Sentiment\n",
      "Date                                                    \n",
      "2025-11-18  -0.507338       0.119248           -0.490013\n",
      "2025-11-19  -0.498653       0.025994           -0.486453\n",
      "2025-11-20  -0.511872       0.001138           -0.492954\n",
      "2025-11-25  -0.488513       0.021033           -0.439336\n",
      "2025-12-02  -0.515744      -0.257131           -0.671667\n"
     ]
    }
   ],
   "source": [
    "# Create a copy to avoid SettingWithCopy warnings\n",
    "df_enhanced = df_clean.copy()\n",
    "\n",
    "# --- 1. Smoothing (Trend Detection) ---\n",
    "# 3-Day and 7-Day Rolling Average of Sentiment\n",
    "df_enhanced['Sent_MA_3'] = df_enhanced['Sentiment_Tone'].rolling(window=3).mean()\n",
    "df_enhanced['Sent_MA_7'] = df_enhanced['Sentiment_Tone'].rolling(window=7).mean()\n",
    "\n",
    "# --- 2. Momentum (Change in Mood) ---\n",
    "# Is the news getting better or worse?\n",
    "df_enhanced['Sent_Momentum'] = df_enhanced['Sentiment_Tone'].diff()\n",
    "\n",
    "# --- 3. Interaction (Volume Weighted Sentiment) ---\n",
    "# Scale Sentiment by Volume (Normalize volume first to avoid huge numbers)\n",
    "vol_mean = df_enhanced['News_Volume'].rolling(window=20).mean()\n",
    "df_enhanced['Relative_Vol'] = df_enhanced['News_Volume'] / vol_mean\n",
    "df_enhanced['Weighted_Sentiment'] = df_enhanced['Sentiment_Tone'] * df_enhanced['Relative_Vol']\n",
    "\n",
    "# --- 4. Volatility Regime ---\n",
    "# Is the news highly conflicted? (High Dispersion)\n",
    "df_enhanced['Dispersion_MA_3'] = df_enhanced['Sentiment_Dispersion'].rolling(window=3).mean()\n",
    "\n",
    "# Drop the NaNs created by rolling windows (first 20 rows)\n",
    "df_enhanced = df_enhanced.dropna()\n",
    "\n",
    "print(\"New Features Created:\")\n",
    "print(df_enhanced[['Sent_MA_7', 'Sent_Momentum', 'Weighted_Sentiment']].tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
