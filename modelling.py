import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, precision_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import warnings
import os

warnings.filterwarnings("ignore")

# === Configuration ===
DATA_PATH = 'results/merged_stooq_gdelt.csv'
RESULTS_DIR = 'results'
SEQ_LEN = 10  # Sequence length for LSTM
TEST_SIZE_RATIO = 0.2
RANDOM_SEED = 42

# === Reproducibility ===
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(RANDOM_SEED)

# Ensure results directory exists
os.makedirs(RESULTS_DIR, exist_ok=True)

def save_plot(fig, filename):
    path = os.path.join(RESULTS_DIR, filename)
    fig.savefig(path)
    print(f"Plot saved to {path}")
    plt.close(fig)

def plot_equity_curves(equity_curves, title="Equity Curves"):
    plt.figure(figsize=(12, 6))
    for name, equity in equity_curves.items():
        plt.plot(equity, label=name)
    
    plt.title(title)
    plt.xlabel("Days")
    plt.ylabel("Equity ($)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    save_plot(plt.gcf(), "equity_curves.png")

def plot_feature_importance(importances, title="Feature Importance"):
    plt.figure(figsize=(10, 8))
    # Sort for better visualization
    importances = importances.sort_values(ascending=True)
    plt.barh(importances.index, importances.values)
    plt.title(title)
    plt.xlabel("Importance")
    plt.tight_layout()
    save_plot(plt.gcf(), "feature_importance.png")

def save_metrics(metrics, filename="metrics.csv"):
    df = pd.DataFrame(metrics)
    path = os.path.join(RESULTS_DIR, filename)
    df.to_csv(path, index=False)
    print(f"Metrics saved to {path}")

def load_and_process_data(filepath):
    print(f"Loading data from {filepath}...")
    df = pd.read_csv(filepath)
    df['Date'] = pd.to_datetime(df['Date'])
    df = df.sort_values('Date').set_index('Date')
    
    # Calculate Returns
    df['Return'] = np.log(df['SPX_Close'] / df['SPX_Close'].shift(1))
    
    # Target: Next Day Direction (1 = Up, 0 = Down)
    df['Target'] = (df['Return'].shift(-1) > 0).astype(int)
    
    # Drop NaN from initial shift
    df = df.dropna()
    return df

def create_features(df):
    data = df.copy()
    
    # === Base Market Features ===
    # Lags
    for lag in [1, 2, 3, 5, 10]:
        data[f'Ret_Lag{lag}'] = data['Return'].shift(lag)
    
    # Volatility
    data['Vol_5'] = data['Return'].rolling(5).std()
    data['Vol_20'] = data['Return'].rolling(20).std()
    
    # Momentum / Intraday
    data['Intraday_Move'] = (data['SPX_Close'] - data['SPX_Open']) / data['SPX_Open']

    # Trend (Price relative to MA)
    data['MA_50'] = data['SPX_Close'].rolling(50).mean()
    data['Trend_50'] = data['SPX_Close'] / data['MA_50'] - 1
    
    # Longer Momentum
    data['Mom_20'] = data['SPX_Close'].pct_change(20)
    data['Mom_60'] = data['SPX_Close'].pct_change(60)

    # RSI (14-day)
    delta = data['SPX_Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
    rs = gain / loss
    data['RSI'] = 100 - (100 / (1 + rs))
    
    # === Sentiment Features ===
    # Moving Averages of Sentiment
    data['Sent_MA_3'] = data['News_Sentiment'].rolling(3).mean()
    data['Sent_MA_7'] = data['News_Sentiment'].rolling(7).mean()
    data['Sent_MA_14'] = data['News_Sentiment'].rolling(14).mean()
    
    # Sentiment Volatility
    data['Sent_Vol_5'] = data['News_Volatility'].rolling(5).mean()
    
    # Interaction: Sentiment * Volume (Normalized)
    vol_ma = data['News_Volume'].rolling(20).mean()
    data['News_Vol_Rel'] = data['News_Volume'] / vol_ma
    data['Sent_Impact'] = data['News_Sentiment'] * data['News_Vol_Rel']
    
    # Drop NaNs generated by rolling/shifting
    data = data.dropna()
    return data

def get_feature_sets():
    base_features = [
        'Ret_Lag1', 'Ret_Lag2', 'Ret_Lag3', 'Ret_Lag5', 'Ret_Lag10',
        'Vol_5', 'Vol_20', 'Intraday_Move', 'Trend_50', 'RSI',
        'Mom_20', 'Mom_60'
    ]
    
    sentiment_features = [
        'Sent_MA_3', 'Sent_MA_7', 'Sent_MA_14', 
        'Sent_Vol_5', 'Sent_Impact', 'News_Disagreement'
    ]
    
    return base_features, sentiment_features

# === Models ===

def train_rf(X_train, y_train, X_test, y_test):
    print(f"Class Balance (Train): {y_train.value_counts(normalize=True).to_dict()}")
    model = RandomForestClassifier(
        n_estimators=300,
        max_depth=4, # Shallower tree to capture broad trends
        min_samples_leaf=20, 
        random_state=RANDOM_SEED,
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    probs = model.predict_proba(X_test)[:, 1]
    return preds, probs, model

class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.3):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_dim, 1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :]) # Last time step
        return self.sigmoid(out)

def train_lstm(X_train, y_train, X_test, y_test, input_dim):
    # Convert to sequences
    def create_sequences(X, y, seq_len):
        xs, ys = [], []
        for i in range(len(X) - seq_len):
            xs.append(X[i:(i + seq_len)])
            ys.append(y[i + seq_len])
        return np.array(xs), np.array(ys)

    # Scale data
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values, SEQ_LEN)
    X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, SEQ_LEN)
    
    # Tensors
    train_data = TensorDataset(torch.FloatTensor(X_train_seq), torch.FloatTensor(y_train_seq))
    train_loader = DataLoader(train_data, batch_size=32, shuffle=False) # Shuffle=False for time series usually, but for training batches it's debatable. Let's keep False to be safe on order, though for gradients it matters less. Actually, shuffling batches is fine, but let's stick to standard.
    
    model = LSTMModel(input_dim)
    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # Train
    model.train()
    for epoch in range(50): 
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            y_pred = model(X_batch)
            loss = criterion(y_pred, y_batch.unsqueeze(1))
            loss.backward()
            optimizer.step()
            
    # Predict
    model.eval()
    with torch.no_grad():
        probs = model(torch.FloatTensor(X_test_seq)).numpy().flatten()
        preds = (probs > 0.5).astype(int)
        
    return preds, probs, model

def backtest_strategy(returns, preds, strategy_name):
    # Align lengths (LSTM loses SEQ_LEN data points)
    min_len = min(len(returns), len(preds))
    returns = returns[-min_len:]
    preds = preds[-min_len:]
    
    equity = [1.0]
    for i in range(len(returns)):
        ret = returns[i]
        # Strategy: If pred=1 (Up), Buy. If pred=0 (Down), Cash (0 return).
        # Simple Long-Only strategy based on signal.
        pos = 1 if preds[i] == 1 else 0
        equity.append(equity[-1] * (1 + pos * ret))
        
    total_return = (equity[-1] - 1) * 100
    return equity, total_return

def run_experiment():
    # 1. Load
    df = load_and_process_data(DATA_PATH)
    df = create_features(df)
    
    base_feats, sent_feats = get_feature_sets()
    all_feats = base_feats + sent_feats
    
    # 2. Split
    train_size = int(len(df) * (1 - TEST_SIZE_RATIO))
    train_df = df.iloc[:train_size]
    test_df = df.iloc[train_size:]
    
    print(f"Train samples: {len(train_df)}, Test samples: {len(test_df)}")
    print(f"Test Date Range: {test_df.index.min()} to {test_df.index.max()}")
    
    results = []
    
    # === Experiment 1: Base Model (RF) ===
    print("\n--- Running Base Model (RF) ---")
    preds_base_rf, probs_base_rf, _ = train_rf(
        train_df[base_feats], train_df['Target'],
        test_df[base_feats], test_df['Target']
    )
    acc_base_rf = accuracy_score(test_df['Target'], preds_base_rf)
    print(f"Base RF Accuracy: {acc_base_rf:.2%}")
    
    # === Experiment 2: Sentiment Model (RF) ===
    print("\n--- Running Sentiment Model (RF) ---")
    preds_sent_rf, probs_sent_rf, model_sent_rf = train_rf(
        train_df[all_feats], train_df['Target'],
        test_df[all_feats], test_df['Target']
    )
    acc_sent_rf = accuracy_score(test_df['Target'], preds_sent_rf)
    print(f"Sentiment RF Accuracy: {acc_sent_rf:.2%}")
    
    # Feature Importance
    importances = pd.Series(model_sent_rf.feature_importances_, index=all_feats).sort_values(ascending=False)
    print("\nTop 5 Features (Sentiment RF):")
    print(importances.head(5))

    # === Experiment 3: Base Model (LSTM) ===
    print("\n--- Running Base Model (LSTM) ---")
    # Note: LSTM predictions will be shorter by SEQ_LEN
    preds_base_lstm, probs_base_lstm, _ = train_lstm(
        train_df[base_feats], train_df['Target'],
        test_df[base_feats], test_df['Target'],
        input_dim=len(base_feats)
    )
    # Align targets for LSTM
    y_test_lstm = test_df['Target'].iloc[SEQ_LEN:].values
    acc_base_lstm = accuracy_score(y_test_lstm, preds_base_lstm)
    print(f"Base LSTM Accuracy: {acc_base_lstm:.2%}")

    # === Experiment 4: Sentiment Model (LSTM) ===
    print("\n--- Running Sentiment Model (LSTM) ---")
    preds_sent_lstm, probs_sent_lstm, _ = train_lstm(
        train_df[all_feats], train_df['Target'],
        test_df[all_feats], test_df['Target'],
        input_dim=len(all_feats)
    )
    acc_sent_lstm = accuracy_score(y_test_lstm, preds_sent_lstm)
    print(f"Sentiment LSTM Accuracy: {acc_sent_lstm:.2%}")

    # === Backtesting ===
    print("\n=== Backtest Results (Cumulative Return) ===")
    actual_returns = test_df['Return'].values
    
    equity_curves = {}
    metrics = []

    # Buy & Hold
    equity_bh = [1.0]
    for r in actual_returns:
        equity_bh.append(equity_bh[-1] * (1 + r))
    bh_return = (equity_bh[-1] - 1) * 100
    print(f"Buy & Hold: {bh_return:.2f}%")
    equity_curves['Buy & Hold'] = equity_bh
    metrics.append({'Model': 'Buy & Hold', 'Accuracy': np.nan, 'Return': bh_return})
    
    # RF Base
    equity_base_rf, ret_base_rf = backtest_strategy(actual_returns, preds_base_rf, "Base RF")
    print(f"Base RF:    {ret_base_rf:.2f}%")
    equity_curves['Base RF'] = equity_base_rf
    metrics.append({'Model': 'Base RF', 'Accuracy': acc_base_rf, 'Return': ret_base_rf})
    
    # RF Sentiment
    equity_sent_rf, ret_sent_rf = backtest_strategy(actual_returns, preds_sent_rf, "Sent RF")
    print(f"Sent RF:    {ret_sent_rf:.2f}%")
    equity_curves['Sent RF'] = equity_sent_rf
    metrics.append({'Model': 'Sent RF', 'Accuracy': acc_sent_rf, 'Return': ret_sent_rf})
    
    # LSTM Base
    # Adjust returns for LSTM (skip first SEQ_LEN)
    lstm_returns = actual_returns[SEQ_LEN:]
    equity_base_lstm, ret_base_lstm = backtest_strategy(lstm_returns, preds_base_lstm, "Base LSTM")
    print(f"Base LSTM:  {ret_base_lstm:.2f}%")
    # Pad LSTM equity to match length for plotting (optional, or just plot aligned)
    # For simplicity, we'll just plot what we have, but let's align start
    equity_curves['Base LSTM'] = [np.nan]*SEQ_LEN + equity_base_lstm
    metrics.append({'Model': 'Base LSTM', 'Accuracy': acc_base_lstm, 'Return': ret_base_lstm})
    
    # LSTM Sentiment
    equity_sent_lstm, ret_sent_lstm = backtest_strategy(lstm_returns, preds_sent_lstm, "Sent LSTM")
    print(f"Sent LSTM:  {ret_sent_lstm:.2f}%")
    equity_curves['Sent LSTM'] = [np.nan]*SEQ_LEN + equity_sent_lstm
    metrics.append({'Model': 'Sent LSTM', 'Accuracy': acc_sent_lstm, 'Return': ret_sent_lstm})

    # === Reporting & Plotting ===
    print("\n=== Generating Reports ===")
    
    # 1. Save Metrics
    save_metrics(metrics)
    
    # 2. Plot Equity Curves
    plot_equity_curves(equity_curves)
    
    # 3. Plot Feature Importance (RF Sentiment)
    plot_feature_importance(importances, title="Feature Importance (RF Sentiment)")

    # === Conclusion ===
    print("\n=== Conclusion ===")
    if ret_sent_rf > ret_base_rf:
        print("✅ Random Forest: Sentiment features IMPROVED returns.")
    else:
        print("❌ Random Forest: Sentiment features DID NOT improve returns.")
        
    if ret_sent_lstm > ret_base_lstm:
        print("✅ LSTM: Sentiment features IMPROVED returns.")
    else:
        print("❌ LSTM: Sentiment features DID NOT improve returns.")

if __name__ == "__main__":
    run_experiment()
